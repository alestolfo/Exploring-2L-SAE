{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformer_lens\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import pprint\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from IPython.display import HTML\n",
    "from functools import partial\n",
    "import tqdm.notebook as tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/workspace/cache/\"\n",
    "from neel.imports import *\n",
    "from neel_plotly import *\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Loading\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gelu-2l\")\n",
    "\n",
    "n_layers = model.cfg.n_layers\n",
    "d_model = model.cfg.d_model\n",
    "n_heads = model.cfg.n_heads\n",
    "d_head = model.cfg.d_head\n",
    "d_mlp = model.cfg.d_mlp\n",
    "d_vocab = model.cfg.d_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
    "SAVE_DIR = Path(\"/workspace/1L-Sparse-Autoencoder/checkpoints\")\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        d_hidden = cfg[\"dict_size\"]\n",
    "        l1_coeff = cfg[\"l1_coeff\"]\n",
    "        dtype = DTYPES[cfg[\"enc_dtype\"]]\n",
    "        torch.manual_seed(cfg[\"seed\"])\n",
    "        self.W_enc = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(cfg[\"act_size\"], d_hidden, dtype=dtype)))\n",
    "        self.W_dec = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_hidden, cfg[\"act_size\"], dtype=dtype)))\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_hidden, dtype=dtype))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(cfg[\"act_size\"], dtype=dtype))\n",
    "\n",
    "        self.W_dec.data[:] = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        self.d_hidden = d_hidden\n",
    "        self.l1_coeff = l1_coeff\n",
    "\n",
    "        self.to(cfg[\"device\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_cent = x - self.b_dec\n",
    "        acts = F.relu(x_cent @ self.W_enc + self.b_enc)\n",
    "        x_reconstruct = acts @ self.W_dec + self.b_dec\n",
    "        l2_loss = (x_reconstruct.float() - x.float()).pow(2).sum(-1).mean(0)\n",
    "        l1_loss = self.l1_coeff * (acts.float().abs().sum())\n",
    "        loss = l2_loss + l1_loss\n",
    "        return loss, x_reconstruct, acts, l2_loss, l1_loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def make_decoder_weights_and_grad_unit_norm(self):\n",
    "        W_dec_normed = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
    "        W_dec_grad_proj = (self.W_dec.grad * W_dec_normed).sum(-1, keepdim=True) * W_dec_normed\n",
    "        self.W_dec.grad -= W_dec_grad_proj\n",
    "        # Bugfix(?) for ensuring W_dec retains unit norm, this was not there when I trained my original autoencoders.\n",
    "        self.W_dec.data = W_dec_normed\n",
    "    \n",
    "    def get_version(self):\n",
    "        version_list = [int(file.name.split(\".\")[0]) for file in list(SAVE_DIR.iterdir()) if \"pt\" in str(file)]\n",
    "        if len(version_list):\n",
    "            return 1+max(version_list)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def save(self):\n",
    "        version = self.get_version()\n",
    "        torch.save(self.state_dict(), SAVE_DIR/(str(version)+\".pt\"))\n",
    "        with open(SAVE_DIR/(str(version)+\"_cfg.json\"), \"w\") as f:\n",
    "            json.dump(cfg, f)\n",
    "        print(\"Saved as version\", version)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, version):\n",
    "        cfg = (json.load(open(SAVE_DIR/(str(version)+\"_cfg.json\"), \"r\")))\n",
    "        pprint.pprint(cfg)\n",
    "        self = cls(cfg=cfg)\n",
    "        self.load_state_dict(torch.load(SAVE_DIR/(str(version)+\".pt\")))\n",
    "        return self\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_hf(cls, version, device_override=None):\n",
    "        \"\"\"\n",
    "        Loads the saved autoencoder from HuggingFace. \n",
    "        \n",
    "        Version is expected to be an int, or \"run1\" or \"run2\"\n",
    "\n",
    "        version 25 is the final checkpoint of the first autoencoder run,\n",
    "        version 47 is the final checkpoint of the second autoencoder run.\n",
    "        \"\"\"\n",
    "        if version==\"run1\":\n",
    "            version = 25\n",
    "        elif version==\"run2\":\n",
    "            version = 47\n",
    "        \n",
    "        cfg = utils.download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version}_cfg.json\")\n",
    "        if device_override is not None:\n",
    "            cfg[\"device\"] = device_override\n",
    "\n",
    "        pprint.pprint(cfg)\n",
    "        self = cls(cfg=cfg)\n",
    "        self.load_state_dict(utils.download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version}.pt\", force_is_torch=True))\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder0 = AutoEncoder.load_from_hf(\"gelu-2l_L0_16384_mlp_out_51\", \"cuda\")\n",
    "encoder1 = AutoEncoder.load_from_hf(\"gelu-2l_L1_16384_mlp_out_50\", \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = load_dataset(\"NeelNanda/c4-10k\", split=\"train\")\n",
    "tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
    "tokenized_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"vscode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     example_tokens = tokenized_data[:200][\"tokens\"]\n",
    "#     logits, cache = model.run_with_cache(example_tokens)\n",
    "#     per_token_loss = model.loss_fn(logits, example_tokens, True)\n",
    "#     imshow(per_token_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# original_mlp_out = cache[\"mlp_out\", 1]\n",
    "# loss, reconstr_mlp_out, hidden_acts, l2_loss, l1_loss = encoder1(original_mlp_out)\n",
    "# def reconstr_hook(mlp_out, hook, new_mlp_out):\n",
    "#     return new_mlp_out\n",
    "# def zero_abl_hook(mlp_out, hook):\n",
    "#     return torch.zeros_like(mlp_out)\n",
    "# print(\"reconstr\", model.run_with_hooks(example_tokens, fwd_hooks=[(utils.get_act_name(\"mlp_out\", 1), partial(reconstr_hook, new_mlp_out=reconstr_mlp_out))], return_type=\"loss\"))\n",
    "# print(\"Orig\", model(example_tokens, return_type=\"loss\"))\n",
    "# print(\"Zero\", model.run_with_hooks(example_tokens, return_type=\"loss\", fwd_hooks=[(utils.get_act_name(\"mlp_out\", 1), zero_abl_hook)]))\n",
    "# # %%\n",
    "\n",
    "# original_mlp_out = cache[\"mlp_out\", 0]\n",
    "# loss, reconstr_mlp_out, hidden_acts, l2_loss, l1_loss = encoder0(original_mlp_out)\n",
    "# def reconstr_hook(mlp_out, hook, new_mlp_out):\n",
    "#     return new_mlp_out\n",
    "# def zero_abl_hook(mlp_out, hook):\n",
    "#     return torch.zeros_like(mlp_out)\n",
    "# print(\"reconstr\", model.run_with_hooks(example_tokens, fwd_hooks=[(utils.get_act_name(\"mlp_out\", 0), partial(reconstr_hook, new_mlp_out=reconstr_mlp_out))], return_type=\"loss\"))\n",
    "# print(\"Orig\", model(example_tokens, return_type=\"loss\"))\n",
    "# print(\"Zero\", model.run_with_hooks(example_tokens, return_type=\"loss\", fwd_hooks=[(utils.get_act_name(\"mlp_out\", 0), zero_abl_hook)]))\n",
    "# # %%\n",
    "# orig_logits = model(example_tokens)\n",
    "# orig_ptl = model.loss_fn(orig_logits, example_tokens, True)\n",
    "\n",
    "# zero_logits = model.run_with_hooks(example_tokens, return_type=\"logits\", fwd_hooks=[(utils.get_act_name(\"mlp_out\", 0), zero_abl_hook)])\n",
    "# zero_ptl = model.loss_fn(zero_logits, example_tokens, True)\n",
    "\n",
    "# recons_logits = model.run_with_hooks(example_tokens, fwd_hooks=[(utils.get_act_name(\"mlp_out\", 0), partial(reconstr_hook, new_mlp_out=reconstr_mlp_out))], return_type=\"logits\")\n",
    "# recons_ptl = model.loss_fn(recons_logits, example_tokens, True)\n",
    "# # %%\n",
    "# histogram(recons_ptl.flatten())\n",
    "# # %%\n",
    "# scatter(x=(recons_ptl-orig_ptl).flatten(), y=(zero_ptl-orig_ptl).flatten())\n",
    "# delta_ptl = recons_ptl - orig_ptl\n",
    "# histogram(delta_ptl.flatten(), marginal=\"box\")\n",
    "# # %%\n",
    "# scipy.stats.kurtosis(to_numpy(delta_ptl).flatten())\n",
    "# # %%\n",
    "# token_df = nutils.make_token_df(example_tokens).query(\"pos>=1\")\n",
    "# token_df[\"delta_ptl\"] = to_numpy(delta_ptl.flatten())\n",
    "\n",
    "# # %%\n",
    "# display(token_df.sort_values(\"delta_ptl\", ascending=False).head(20))\n",
    "# display(token_df.sort_values(\"delta_ptl\", ascending=True).head(20))\n",
    "# # %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    virtual_weights = encoder0.W_dec @ model.W_in[1] @ model.W_out[1] @ encoder1.W_enc\n",
    "    virtual_weights.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cfg.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder1.W_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "histogram(virtual_weights.flatten()[::1001])\n",
    "# %%\n",
    "with torch.no_grad():\n",
    "    neuron2neuron = model.W_out[0] @ model.W_in[1]\n",
    "# histogram(neuron2neuron.flatten()[::101])\n",
    "# # %%\n",
    "# histogram(virtual_weights.mean(0), title=\"Ave by end feature\")\n",
    "# histogram(virtual_weights.mean(1), title=\"Ave by start feature\")\n",
    "# histogram(virtual_weights.median(0).values, title=\"Median by end feature\")\n",
    "# histogram(virtual_weights.median(1).values, title=\"Median by start feature\")\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-Occurence investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START HERE!\n",
    "example_tokens = tokenized_data[:800][\"tokens\"]\n",
    "with torch.no_grad():\n",
    "    _, cache = model.run_with_cache(example_tokens, stop_at_layer=2, names_filter=lambda x: \"mlp_out\" in x)\n",
    "    loss, recons_mlp_out0, hidden_acts0, l2_loss, l1_loss = encoder0(cache[\"mlp_out\", 0])\n",
    "    loss, recons_mlp_out1, hidden_acts1, l2_loss, l1_loss = encoder1(cache[\"mlp_out\", 1])\n",
    "\n",
    "try:\n",
    "    hidden_acts0 = hidden_acts0[:, 1:, :]\n",
    "    hidden_acts0 = einops.rearrange(hidden_acts0, \"batch pos d_enc -> (batch pos) d_enc\")\n",
    "    hidden_acts1 = hidden_acts1[:, 1:, :]\n",
    "    hidden_acts1 = einops.rearrange(hidden_acts1, \"batch pos d_enc -> (batch pos) d_enc\")\n",
    "except:\n",
    "    print(\"FAILED\")\n",
    "    pass\n",
    "\n",
    "hidden_is_pos0 = hidden_acts0 > 0\n",
    "hidden_is_pos1 = hidden_acts1 > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-Occur in SAEs\n",
    "\n",
    "d_enc = hidden_acts0.shape[-1]\n",
    "cooccur_count = torch.zeros((d_enc, d_enc), device=\"cuda\", dtype=torch.float32)\n",
    "for other_i in tqdm.trange(d_enc):\n",
    "    cooccur_count[:, other_i] = hidden_is_pos0[hidden_is_pos0[:, other_i]].float().sum(0)\n",
    "# %%\n",
    "num_firings0 = hidden_is_pos0.sum(0)\n",
    "cooccur_freq = cooccur_count / torch.maximum(num_firings0[:, None], num_firings0[None, :])\n",
    "# %%\n",
    "# cooccur_count = cooccur_count.float() / hidden_acts0.shape[0]\n",
    "# %%\n",
    "histogram(cooccur_freq[cooccur_freq>0.1], log_y=True)\n",
    "\n",
    "# set nan to 0\n",
    "cooccur_freq[cooccur_freq.isnan()] = 0.\n",
    "# set diag to 0\n",
    "cooccur_freq.fill_diagonal_(0)\n",
    "\n",
    "# set to 0 any rows or columns where individual features don't fire enough\n",
    "cooccur_freq[num_firings0 < 10, :] = 0\n",
    "cooccur_freq[:, num_firings0 < 10] = 0\n",
    "\n",
    "val, ind = cooccur_freq.flatten().topk(100)\n",
    "\n",
    "start_topk_ind = (ind // d_enc)\n",
    "other_topk_ind = (ind % d_enc)\n",
    "# print(val)\n",
    "# print(start_topk_ind)\n",
    "# print(other_topk_ind)\n",
    "\n",
    "# print(\"earlier_features\", num_firings0[start_topk_ind])\n",
    "# print(\"other features\", num_firings0[other_topk_ind])\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"start_topk_ind\": start_topk_ind.cpu().numpy(),\n",
    "        \"other_topk_ind\": other_topk_ind.cpu().numpy(),\n",
    "        \"val\": val.cpu().numpy(),\n",
    "        \"earlier_features\": num_firings0[start_topk_ind].cpu().numpy(),\n",
    "        \"other_features\": num_firings0[other_topk_ind].cpu().numpy(),\n",
    "    }\n",
    ")\n",
    "\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-Occur in SAEs\n",
    "\n",
    "d_enc = hidden_acts0.shape[-1]\n",
    "cooccur_count = torch.zeros((d_enc, d_enc), device=\"cuda\", dtype=torch.float32)\n",
    "for other_i in tqdm.trange(d_enc):\n",
    "    cooccur_count[:, other_i] = hidden_is_pos1[hidden_is_pos1[:, other_i]].float().sum(0)\n",
    "# %%\n",
    "num_firings1 = hidden_is_pos1.sum(0)\n",
    "cooccur_freq = cooccur_count / torch.maximum(num_firings1[:, None], num_firings1[None, :])\n",
    "# %%\n",
    "# cooccur_count = cooccur_count.float() / hidden_acts0.shape[0]\n",
    "# %%\n",
    "histogram(cooccur_freq[cooccur_freq>0.1], log_y=True)\n",
    "\n",
    "# set nan to 0\n",
    "cooccur_freq[cooccur_freq.isnan()] = 0.\n",
    "# set diag to 0\n",
    "cooccur_freq.fill_diagonal_(0)\n",
    "\n",
    "# set to 0 any rows or columns where individual features don't fire enough\n",
    "cooccur_freq[num_firings1 < 10, :] = 0\n",
    "cooccur_freq[:, num_firings1 < 10] = 0\n",
    "\n",
    "val, ind = cooccur_freq.flatten().topk(100)\n",
    "\n",
    "start_topk_ind = (ind // d_enc)\n",
    "other_topk_ind = (ind % d_enc)\n",
    "# print(val)\n",
    "# print(start_topk_ind)\n",
    "# print(other_topk_ind)\n",
    "\n",
    "# print(\"earlier_features\", num_firings0[start_topk_ind])\n",
    "# print(\"other features\", num_firings0[other_topk_ind])\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"start_topk_ind\": start_topk_ind.cpu().numpy(),\n",
    "        \"other_topk_ind\": other_topk_ind.cpu().numpy(),\n",
    "        \"val\": val.cpu().numpy(),\n",
    "        \"earlier_features\": num_firings1[start_topk_ind].cpu().numpy(),\n",
    "        \"other_features\": num_firings1[other_topk_ind].cpu().numpy(),\n",
    "    }\n",
    ")\n",
    "print(df[\"start_topk_ind\"].value_counts())\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 6269\n",
    "# get rows where feature is in start_topk_ind\n",
    "df.query(\"start_topk_ind==@feature\").sort_values(\"val\", ascending=False).head(20)\n",
    "# get rows where feature is in other_topk_ind\n",
    "df.query(\"other_topk_ind==@feature\").sort_values(\"val\", ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_token = example_tokens[:, 1:].flatten() == model.to_single_token(\" big\")\n",
    "# line(hidden_is_pos0[is_token].float().mean(0))\n",
    "# line(hidden_is_pos1[is_token].float().mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_ids = [3698"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_id_1 = 12989\n",
    "feature_id_2 = 14809\n",
    "feature_id_3 = 15690\n",
    "token_df = nutils.make_token_df(example_tokens).query(\"pos>=1\")\n",
    "token_df[\"val1\"] = to_numpy(hidden_acts0[:, feature_id_1])\n",
    "token_df[\"val2\"] = to_numpy(hidden_acts0[:, feature_id_2])\n",
    "token_df[\"val3\"] = to_numpy(hidden_acts0[:, feature_id_3])\n",
    "pd.set_option('display.max_rows', 40)\n",
    "display(token_df.sort_values(\"val1\", ascending=False).head(20))\n",
    "display(token_df.sort_values(\"val2\", ascending=False).head(20))\n",
    "display(token_df.sort_values(\"val3\", ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# px.scatter(token_df, x=\"val1\", y=\"val2\", hover_data=[\"str_tokens\", \"context\"], title=\"Feature 1 vs Feature 2\")\n",
    "fig = px.scatter_3d(\n",
    "    token_df, x=\"val1\", y=\"val2\", z=\"val3\", \n",
    "    opacity = 0.4,\n",
    "    hover_data=[\"str_tokens\", \"context\"], title=\"Feature 1 vs Feature 2 vs Feature 3\")\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "# make it much larger\n",
    "fig.update_layout(width=800, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Co-Occurence graphically\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_id_1 = 12989\n",
    "feature_id_2 = 14809\n",
    "feature_id_3 = 15690\n",
    "token_df = nutils.make_token_df(example_tokens).query(\"pos>=1\")\n",
    "token_df[\"0_val1\"] = to_numpy(hidden_acts0[:, feature_id_1])\n",
    "token_df[\"0_val2\"] = to_numpy(hidden_acts0[:, feature_id_2])\n",
    "token_df[\"0_val3\"] = to_numpy(hidden_acts0[:, feature_id_3])\n",
    "feature_id_1 = 1245\n",
    "feature_id_2 = 1353\n",
    "feature_id_3 = 9604\n",
    "token_df[\"1_val1\"] = to_numpy(hidden_acts1[:, feature_id_1])\n",
    "token_df[\"1_val2\"] = to_numpy(hidden_acts1[:, feature_id_2])\n",
    "token_df[\"1_val3\"] = to_numpy(hidden_acts1[:, feature_id_3])\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    token_df,\n",
    "    dimensions=[\"0_val1\", \"0_val2\", \"0_val3\", \"1_val1\", \"1_val2\", \"1_val3\"],\n",
    "    # color=\"species\"\n",
    "    hover_data=[\"str_tokens\", \"context\"],\n",
    ")\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "# make it much larger\n",
    "fig.update_layout(width=1200, height=1200)\n",
    "fig.show()\n",
    "\n",
    "# fig = px.scatter_3d(\n",
    "#     token_df, x=\"0_val1\", y=\"0_val2\", z=\"0_val3\", \n",
    "#     color=\"1_val3\",\n",
    "#     hover_data=[\"str_tokens\", \"context\"], title=\"Feature 1 vs Feature 2 vs Feature 3\")\n",
    "# fig.update_traces(marker=dict(size=5))\n",
    "# # make it much larger\n",
    "# fig.update_layout(width=800, height=800)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-Occurence between SAE's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-Occur between SAEs\n",
    "\n",
    "d_enc = hidden_acts0.shape[-1]\n",
    "cooccur_count = torch.zeros((d_enc, d_enc), device=\"cuda\", dtype=torch.float32)\n",
    "for end_i in tqdm.trange(d_enc):\n",
    "    cooccur_count[:, end_i] = hidden_is_pos0[hidden_is_pos1[:, end_i]].float().sum(0)\n",
    "# %%\n",
    "num_firings0 = hidden_is_pos0.sum(0)\n",
    "num_firings1 = hidden_is_pos1.sum(0)\n",
    "cooccur_freq = cooccur_count / torch.maximum(num_firings0[:, None], num_firings1[None, :])\n",
    "# %%\n",
    "# cooccur_count = cooccur_count.float() / hidden_acts0.shape[0]\n",
    "# %%\n",
    "histogram(cooccur_freq[cooccur_freq>0.1], log_y=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "cooccur_freq[cooccur_freq.isnan()] = 0.\n",
    "val, ind = cooccur_freq.flatten().topk(10)\n",
    "\n",
    "start_topk_ind = (ind // d_enc)\n",
    "end_topk_ind = (ind % d_enc)\n",
    "print(val)\n",
    "print(start_topk_ind)\n",
    "print(end_topk_ind)\n",
    "\n",
    "print(\"earlier_features\", num_firings0[start_topk_ind])\n",
    "print(\"later features\", num_firings1[end_topk_ind])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dig Into a specific Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "feature_id = 2593\n",
    "layer = 0\n",
    "token_df = nutils.make_token_df(example_tokens).query(\"pos>=1\")\n",
    "token_df[\"val\"] = to_numpy(hidden_acts1[:, feature_id])\n",
    "pd.set_option('display.max_rows', 50)\n",
    "display(token_df.sort_values(\"val\", ascending=False).head(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_weights = encoder1.W_dec[feature_id, :] @ model.W_U\n",
    "vocab_df = nutils.create_vocab_df(logit_weights)\n",
    "vocab_df[\"has_space\"] = vocab_df[\"token\"].apply(lambda x: nutils.SPACE in x)\n",
    "px.histogram(vocab_df, x=\"logit\", color=\"has_space\", barmode=\"overlay\", marginal=\"box\")\n",
    "display(vocab_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_an = example_tokens[:, 1:].flatten() == model.to_single_token(\" an\")\n",
    "line(hidden_is_pos0[is_an].float().mean(0))\n",
    "line(hidden_is_pos1[is_an].float().mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(x=virtual_weights[5740], y=hidden_is_pos1[is_an].float().mean(0), hover=np.arange(d_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_feature_an = 5740\n",
    "line(hidden_acts0[5740])\n",
    "replacement_for_feature = torch.zeros_like(example_tokens).cuda()\n",
    "replacement_for_feature[:, 1:] = hidden_acts0[:, 5740].reshape(600, 127)\n",
    "mlp_out0_diff = replacement_for_feature[:, :, None] * encoder0.W_dec[l0_feature_an]\n",
    "new_end_topk = end_topk_ind[start_topk_ind==5740]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_an_feature(mlp_out, hook):\n",
    "    mlp_out[:, :] -= mlp_out0_diff\n",
    "    return mlp_out\n",
    "    \n",
    "model.reset_hooks()\n",
    "model.blocks[0].hook_mlp_out.add_hook(remove_an_feature)\n",
    "_, new_cache = model.run_with_cache(example_tokens, stop_at_layer=2, names_filter=lambda x: \"mlp_out\" in x)\n",
    "with torch.no_grad():\n",
    "    loss, x_reconstruct, hidden_acts_new, l2_loss, l1_loss = encoder1(new_cache[\"mlp_out\", 1])\n",
    "model.reset_hooks()\n",
    "# %%\n",
    "new_hidden_acts_on_an = hidden_acts_new[:, :, new_end_topk].reshape(-1, 5)[example_tokens.flatten()==model.to_single_token(\" an\"), :]\n",
    "old_hidden_acts_on_an = hidden_acts1[:, new_end_topk][example_tokens[:, 1:].flatten()==model.to_single_token(\" an\"), :]\n",
    "for i in range(5):\n",
    "    scatter(x=old_hidden_acts_on_an[:, i], y=new_hidden_acts_on_an[:, i], hover=np.arange(180), title=new_end_topk[i].item(), include_diag=True, yaxis=\"POst Ablation\", xaxis=\"Pre Ablation\")\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
