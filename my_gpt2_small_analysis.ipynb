{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "import torch as t\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "import torch as t\n",
    "#from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "import einops\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gpt2-small\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\").to('cuda')\n",
    "\n",
    "data = load_dataset(\"stas/openwebtext-10k\", split=\"train\")\n",
    "tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
    "tokenized_data = tokenized_data.shuffle(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import pprint\n",
    "import json \n",
    "import torch.nn.functional as F\n",
    "\n",
    "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
    "SAVE_DIR = Path(\"/workspace/1L-Sparse-Autoencoder/checkpoints\")\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        d_hidden = cfg[\"dict_size\"]\n",
    "        l1_coeff = cfg[\"l1_coeff\"]\n",
    "        dtype = DTYPES[cfg[\"enc_dtype\"]]\n",
    "        torch.manual_seed(cfg[\"seed\"])\n",
    "        self.W_enc = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(cfg[\"act_size\"], d_hidden, dtype=dtype)))\n",
    "        self.W_dec = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_hidden, cfg[\"act_size\"], dtype=dtype)))\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_hidden, dtype=dtype))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(cfg[\"act_size\"], dtype=dtype))\n",
    "\n",
    "        self.W_dec.data[:] = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        self.d_hidden = d_hidden\n",
    "        self.l1_coeff = l1_coeff\n",
    "\n",
    "        self.to(cfg[\"device\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_cent = x - self.b_dec\n",
    "        acts = F.relu(x_cent @ self.W_enc + self.b_enc)\n",
    "        x_reconstruct = acts @ self.W_dec + self.b_dec\n",
    "        l2_loss = (x_reconstruct.float() - x.float()).pow(2).sum(-1).mean(0)\n",
    "        l1_loss = self.l1_coeff * (acts.float().abs().sum())\n",
    "        loss = l2_loss + l1_loss\n",
    "        return loss, x_reconstruct, acts, l2_loss, l1_loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def make_decoder_weights_and_grad_unit_norm(self):\n",
    "        W_dec_normed = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
    "        W_dec_grad_proj = (self.W_dec.grad * W_dec_normed).sum(-1, keepdim=True) * W_dec_normed\n",
    "        self.W_dec.grad -= W_dec_grad_proj\n",
    "        # Bugfix(?) for ensuring W_dec retains unit norm, this was not there when I trained my original autoencoders.\n",
    "        self.W_dec.data = W_dec_normed\n",
    "    \n",
    "    def get_version(self):\n",
    "        version_list = [int(file.name.split(\".\")[0]) for file in list(SAVE_DIR.iterdir()) if \"pt\" in str(file)]\n",
    "        if len(version_list):\n",
    "            return 1+max(version_list)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def save(self):\n",
    "        version = self.get_version()\n",
    "        torch.save(self.state_dict(), SAVE_DIR/(str(version)+\".pt\"))\n",
    "        with open(SAVE_DIR/(str(version)+\"_cfg.json\"), \"w\") as f:\n",
    "            json.dump(cfg, f)\n",
    "        print(\"Saved as version\", version)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, version):\n",
    "        cfg = (json.load(open(SAVE_DIR/(str(version)+\"_cfg.json\"), \"r\")))\n",
    "        pprint.pprint(cfg)\n",
    "        self = cls(cfg=cfg)\n",
    "        self.load_state_dict(torch.load(SAVE_DIR/(str(version)+\".pt\")))\n",
    "        return self\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_hf(cls, version, device_override=None):\n",
    "        \"\"\"\n",
    "        Loads the saved autoencoder from HuggingFace. \n",
    "        \n",
    "        Version is expected to be an int, or \"run1\" or \"run2\"\n",
    "\n",
    "        version 25 is the final checkpoint of the first autoencoder run,\n",
    "        version 47 is the final checkpoint of the second autoencoder run.\n",
    "        \"\"\"\n",
    "        if version==\"run1\":\n",
    "            version = 25\n",
    "        elif version==\"run2\":\n",
    "            version = 47\n",
    "        \n",
    "        cfg = utils.download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version}_cfg.json\")\n",
    "        if device_override is not None:\n",
    "            cfg[\"device\"] = device_override\n",
    "\n",
    "        pprint.pprint(cfg)\n",
    "        self = cls(cfg=cfg)\n",
    "        self.load_state_dict(utils.download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version}.pt\", force_is_torch=True))\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_point, layer = \"resid_pre\", 10\n",
    "dic = utils.download_file_from_hf(\"jacobcd52/gpt2-small-sparse-autoencoders\", f\"gpt2-small_6144_{hook_point}_{layer}.pt\", force_is_torch=True)\n",
    "W_dec , b_dec, W_enc, b_enc = dic[\"W_dec\"], dic[\"b_dec\"], dic[\"W_enc\"], dic[\"b_enc\"]\n",
    "\n",
    "cfg = {\n",
    "    \"dict_size\": 6144,\n",
    "    \"act_size\": 768,\n",
    "    \"l1_coeff\": 0.001,\n",
    "    \"enc_dtype\": \"fp32\",\n",
    "    \"seed\": 0,\n",
    "    \"device\": \"cuda\",\n",
    "    \"model_batch_size\": 1028,\n",
    "}\n",
    "encoder = AutoEncoder(cfg)\n",
    "encoder.load_state_dict(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the reconstruction loss\n",
    "\n",
    "def replacement_hook(mlp_post, hook, encoder):\n",
    "    mlp_post_reconstr = encoder(mlp_post)[1]\n",
    "    return mlp_post_reconstr\n",
    "\n",
    "def mean_ablate_hook(mlp_post, hook):\n",
    "    mlp_post[:] = mlp_post.mean([0, 1])\n",
    "    return mlp_post\n",
    "\n",
    "def zero_ablate_hook(mlp_post, hook):\n",
    "    mlp_post[:] = 0.\n",
    "    return mlp_post\n",
    "\n",
    "@t.no_grad()\n",
    "def get_recons_loss(encoder, all_tokens, num_batches=5, local_encoder=None):\n",
    "    loss_list = []\n",
    "    for i in range(num_batches):\n",
    "        tokens = all_tokens[t.randperm(len(all_tokens))[:cfg[\"model_batch_size\"]]]\n",
    "        loss = model(tokens, return_type=\"loss\")\n",
    "        recons_loss = model.run_with_hooks(tokens, return_type=\"loss\", fwd_hooks=[(utils.get_act_name(\"resid_pre\", 9), partial(replacement_hook, encoder=local_encoder))])\n",
    "        # mean_abl_loss = model.run_with_hooks(tokens, return_type=\"loss\", fwd_hooks=[(utils.get_act_name(\"post\", 0), mean_ablate_hook)])\n",
    "        zero_abl_loss = model.run_with_hooks(tokens, return_type=\"loss\", fwd_hooks=[(utils.get_act_name(\"resid_pre\", 9), zero_ablate_hook)])\n",
    "        loss_list.append((loss, recons_loss, zero_abl_loss))\n",
    "    losses = t.tensor(loss_list)\n",
    "    loss, recons_loss, zero_abl_loss = losses.mean(0).tolist()\n",
    "\n",
    "    print(f\"loss: {loss:.4f}, recons_loss: {recons_loss:.4f}, zero_abl_loss: {zero_abl_loss:.4f}\")\n",
    "    score = ((zero_abl_loss - recons_loss)/(zero_abl_loss - loss))\n",
    "    print(f\"Reconstruction Score: {score:.2%}\")\n",
    "    # print(f\"{((zero_abl_loss - mean_abl_loss)/(zero_abl_loss - loss)).item():.2%}\")\n",
    "    return score, loss, recons_loss, zero_abl_loss\n",
    "\n",
    "# local_encoder = encoder\n",
    "\n",
    "with torch.no_grad():\n",
    "    example_tokens = tokenized_data[:200][\"tokens\"].to(\"cuda\")\n",
    "    logits, cache = model.run_with_cache(example_tokens)\n",
    "    per_token_loss = model.loss_fn(logits, example_tokens, True)\n",
    "    # imshow(per_token_loss)\n",
    "\n",
    "# get_recons_loss(encoder, example_tokens, num_batches=5, local_encoder=local_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_acts(point, layer, dic, num_batches = 1000, minibatch_size = 50):\n",
    "  try:\n",
    "    del feature_acts\n",
    "    del random_feature_acts\n",
    "  except NameError:\n",
    "    pass\n",
    "\n",
    "  # get however many tokens we need\n",
    "  toks = tokenized_data[\"tokens\"][:num_batches]\n",
    "  toks = toks.to(\"cuda\")\n",
    "\n",
    "  # get activations on test tokens at point of interest. Run model on batches of tokens with size [batch_size, 128]. Be careful with RAM.\n",
    "\n",
    "  random_W_enc = t.randn( dic[\"W_enc\"].size() ).cuda()\n",
    "\n",
    "  for i in tqdm.tqdm(range(toks.size(0)//minibatch_size)):\n",
    "    # split toks into minibatch and run model with cache on minibatch\n",
    "    toks_batch = toks[minibatch_size*i : minibatch_size*(i+1), :]\n",
    "    logits, cache = model.run_with_cache(toks_batch, stop_at_layer=layer+1, names_filter=utils.get_act_name(point, layer))\n",
    "    del logits\n",
    "\n",
    "    act_batch = cache[point, layer]\n",
    "    act_batch = act_batch.detach().cuda()\n",
    "    del cache\n",
    "\n",
    "    # get feature acts and random feature acts on this minibatch (fewer random ones to save RAM)\n",
    "    feature_act_batch = t.relu(einops.einsum(act_batch - dic[\"b_dec\"], dic[\"W_enc\"], \"batch seq resid , resid mlp -> batch seq mlp\")  + dic[\"b_enc\"])\n",
    "\n",
    "    random_feature_act_batch = t.relu(einops.einsum(act_batch[:10] - dic[\"b_dec\"], random_W_enc, \"batch seq resid , resid mlp -> batch seq mlp\")  + dic[\"b_enc\"])\n",
    "    random_feature_act_batch = random_feature_act_batch / random_feature_act_batch.norm(dim=-1, keepdim=True) * feature_act_batch[:10].norm(dim=-1, keepdim=True)  #fix normalisation\n",
    "    del act_batch\n",
    "\n",
    "    # append minibatch feature acts to storage variable\n",
    "    if i == 0:  # on first iteration, create feature_acts\n",
    "      feature_acts = feature_act_batch\n",
    "      random_feature_acts = random_feature_act_batch\n",
    "    else:  # then add to it\n",
    "      feature_acts = t.cat([feature_acts, feature_act_batch], dim=0)\n",
    "      random_feature_acts = t.cat([random_feature_acts, random_feature_act_batch], dim=0)\n",
    "\n",
    "    del feature_act_batch\n",
    "    del random_feature_act_batch\n",
    "\n",
    "  # set BOS acts to zero\n",
    "  feature_acts[:, 0, :] = 0\n",
    "  random_feature_acts[:, 0, :] = 0\n",
    "\n",
    "  # flatten [batch n_seq] dimensions\n",
    "  feature_acts = feature_acts.reshape(-1, feature_acts.size(2))\n",
    "  random_feature_acts = random_feature_acts.reshape(-1, random_feature_acts.size(2))\n",
    "\n",
    "  print(\"feature_acts has size:\", feature_acts.size())\n",
    "\n",
    "  return toks, feature_acts, random_feature_acts\n",
    "\n",
    "\n",
    "# toks, feature_acts, random_feature_acts = get_feature_acts(\"resid_pre\", 9, encoder.state_dict(), num_batches = 1000, minibatch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_metric = (feature_acts > 0).float().mean(0) * 128\n",
    "px.histogram(l0_metric.cpu().numpy(), nbins=100, title=\"L0 Metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_density = (feature_acts > 0).float().mean(0).detach().cpu()\n",
    "feature_density_log = np.log10(feature_density.numpy() + 1e-8)\n",
    "px.histogram(feature_density_log, title=\"Feature Density\", labels={\"value\": \"Density\", \"variable\": \"Feature\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_is_pos0 = feature_acts > 0\n",
    "d_enc = feature_acts.shape[-1]\n",
    "cooccur_count = torch.zeros((d_enc, d_enc), device=\"cuda\", dtype=torch.float32)\n",
    "for end_i in tqdm.trange(d_enc):\n",
    "    cooccur_count[:, end_i] = hidden_is_pos0[hidden_is_pos0[:, end_i]].float().sum(0)\n",
    "# %%\n",
    "num_firings0 = hidden_is_pos0.sum(0)\n",
    "cooccur_freq = cooccur_count / torch.maximum(num_firings0[:, None], num_firings0[None, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set nan to 0\n",
    "cooccur_freq[cooccur_freq.isnan()] = 0.\n",
    "# set diag to 0\n",
    "cooccur_freq.fill_diagonal_(0)\n",
    "# fill dense features with 0 \n",
    "cooccur_freq[feature_density_log > -2,:] = 0\n",
    "# cooccur_freq[:] = 0.\n",
    "\n",
    "val, ind = cooccur_freq.flatten().topk(500)\n",
    "\n",
    "\n",
    "start_topk_ind = (ind // d_enc)\n",
    "other_topk_ind = (ind % d_enc)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"start_topk_ind\": start_topk_ind.cpu().numpy(),\n",
    "        \"other_topk_ind\": other_topk_ind.cpu().numpy(),\n",
    "        \"val\": val.cpu().numpy(),\n",
    "        \"earlier_features\": num_firings0[start_topk_ind].cpu().numpy(),\n",
    "        \"other_features\": num_firings0[other_topk_ind].cpu().numpy(),\n",
    "        \"start_top_k_density\": feature_density[start_topk_ind.cpu()].cpu().numpy(),\n",
    "        \"other_top_k_density\": feature_density[other_topk_ind.cpu()].cpu().numpy(),\n",
    "        \"feature_density_ratio\": feature_density[start_topk_ind.cpu()].cpu().numpy() / feature_density[other_topk_ind.cpu()].cpu().numpy(),\n",
    "    }\n",
    ")\n",
    "\n",
    "df.sample(40)\n",
    "# # %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# cooccur_count = cooccur_count.float() / hidden_acts0.shape[0]\n",
    "# %%\n",
    "fig = px.histogram(cooccur_freq[cooccur_freq>0.3].detach().cpu(), log_y=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = nutils.make_token_df(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  neel.utils as nutils\n",
    "feature_id = 2271\t\n",
    "token_df[\"val1\"] = (feature_acts[:, feature_id]).detach().cpu().numpy()\n",
    "feature_id = 2158\t\n",
    "token_df[\"val2\"] = (feature_acts[:, feature_id]).detach().cpu().numpy()\n",
    "pd.set_option('display.max_rows', 50)\n",
    "display(token_df.sort_values(\"val1\", ascending=False).head(30))\n",
    "display(token_df.sort_values(\"val2\", ascending=False).head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(token_df[(token_df.val1 >0) | (token_df.val2 >0) ], x=\"val1\", y=\"val2\", hover_data=[\"str_tokens\", \"context\"], title=\"Feature 1 vs Feature 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual Weight for semantics of Copy Suprresions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_point, layer = \"resid_pre\", 10\n",
    "dic = utils.download_file_from_hf(\"jacobcd52/gpt2-small-sparse-autoencoders\", f\"gpt2-small_6144_{hook_point}_{layer}.pt\", force_is_torch=True)\n",
    "W_dec , b_dec, W_enc, b_enc = dic[\"W_dec\"], dic[\"b_dec\"], dic[\"W_enc\"], dic[\"b_enc\"]\n",
    "\n",
    "cfg = {\n",
    "    \"dict_size\": 6144,\n",
    "    \"act_size\": 768,\n",
    "    \"l1_coeff\": 0.001,\n",
    "    \"enc_dtype\": \"fp32\",\n",
    "    \"seed\": 0,\n",
    "    \"device\": \"cuda\",\n",
    "    \"model_batch_size\": 1028,\n",
    "}\n",
    "encoder_10 = AutoEncoder(cfg)\n",
    "encoder_10.load_state_dict(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_point, layer = \"resid_pre\", 11\n",
    "dic = utils.download_file_from_hf(\"jacobcd52/gpt2-small-sparse-autoencoders\", f\"gpt2-small_6144_{hook_point}_{layer}.pt\", force_is_torch=True)\n",
    "W_dec , b_dec, W_enc, b_enc = dic[\"W_dec\"], dic[\"b_dec\"], dic[\"W_enc\"], dic[\"b_enc\"]\n",
    "\n",
    "cfg = {\n",
    "    \"dict_size\": 6144,\n",
    "    \"act_size\": 768,\n",
    "    \"l1_coeff\": 0.001,\n",
    "    \"enc_dtype\": \"fp32\",\n",
    "    \"seed\": 0,\n",
    "    \"device\": \"cuda\",\n",
    "    \"model_batch_size\": 1028,\n",
    "}\n",
    "encoder_11= AutoEncoder(cfg)\n",
    "encoder_11.load_state_dict(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_V = torch.stack([block.attn.W_V for block in model.blocks])\n",
    "W_0 = torch.stack([block.attn.W_O for block in model.blocks])\n",
    "W_V = W_V - W_V.mean(-1, keepdim=True)\n",
    "W_0 = W_0 - W_0.mean(-1, keepdim=True)\n",
    "\n",
    "# inner OV circuits.\n",
    "W_OV = torch.einsum(\"lhmd,lhdn->lhmn\", W_V, W_0)\n",
    "W_OV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_10.W_dec.detach().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_dec_10 = encoder_10.W_dec.detach()\n",
    "W_dec_10 = W_dec_10 - W_dec_10.mean(0)\n",
    "W_dec_11 = encoder_11.W_dec.detach()\n",
    "W_dec_11 = W_dec_11 - W_dec_11.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_enc_10 = encoder_10.W_enc.detach()\n",
    "W_enc_10 = W_enc_10 - W_enc_10.mean(0)\n",
    "W_enc_11 = encoder_11.W_enc.detach()\n",
    "W_enc_11 = W_enc_11 - W_enc_11.mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_weights = W_enc_10.T @ W_OV[10] @ W_enc_11\n",
    "virtual_weights = virtual_weights.detach().cpu() \n",
    "virtual_weights.shape # feature to feature score per head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_weights.flatten(1,2)[:,:1000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.violin(virtual_weights.flatten(1,2)[:,:3000].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar((virtual_weights < -2).sum(dim=(1,2)).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = virtual_weights[10].flatten() > 1\n",
    "print(mask.sum()) # per head\n",
    "virtual_weights_for_plotting = virtual_weights[10].flatten()[mask].detach().cpu().numpy()\n",
    "px.histogram(virtual_weights_for_plotting, title=\"Virtual Weights\", log_y=True).show()\n",
    "\n",
    "val, ind = virtual_weights[10].flatten().topk(10)\n",
    "\n",
    "d_enc = virtual_weights.shape[-1]\n",
    "start_topk_ind = (ind // d_enc)\n",
    "end_topk_ind = (ind % d_enc)\n",
    "\n",
    "feature_density_10 = (feature_acts_10 > 0).float().mean(0).detach().cpu()\n",
    "feature_density_11 = (feature_acts_11 > 0).float().mean(0).detach().cpu()\n",
    "\n",
    "\n",
    "print(val)\n",
    "print(start_topk_ind)\n",
    "print(end_topk_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = virtual_weights[10].flatten() <-1\n",
    "print(mask.sum()) # per head\n",
    "virtual_weights_for_plotting = virtual_weights[10].flatten()[mask].detach().cpu().numpy()\n",
    "px.histogram(virtual_weights_for_plotting, title=\"Virtual Weights\", log_y=True).show()\n",
    "\n",
    "val, ind = virtual_weights[10].flatten().topk(10, largest=False)\n",
    "\n",
    "d_enc = virtual_weights.shape[-1]\n",
    "start_topk_ind = (ind // d_enc)\n",
    "end_topk_ind = (ind % d_enc)\n",
    "print(val)\n",
    "print(start_topk_ind)\n",
    "print(end_topk_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_10, feature_acts_10, _ = get_feature_acts(\"resid_pre\", 10, encoder_10.state_dict(), num_batches = 512, minibatch_size = 64)\n",
    "toks_11, feature_acts_11, _ = get_feature_acts(\"resid_pre\", 11, encoder_11.state_dict(), num_batches = 512, minibatch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert toks_10.equal(toks_11)\n",
    "# del cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val, ind = virtual_weights[10].flatten().topk(10)\n",
    "\n",
    "d_enc = virtual_weights.shape[-1]\n",
    "start_topk_ind = (ind // d_enc)\n",
    "end_topk_ind = (ind % d_enc)\n",
    "\n",
    "feature_density_10 = (feature_acts_10 > 0).float().mean(0).detach().cpu()\n",
    "feature_density_11 = (feature_acts_11 > 0).float().mean(0).detach().cpu()\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"start_topk_ind\": start_topk_ind.cpu().numpy(),\n",
    "        \"other_topk_ind\": end_topk_ind.cpu().numpy(),\n",
    "        \"val\": val.detach().cpu().numpy(),\n",
    "        \"start_top_k_density\": feature_density_10[start_topk_ind.cpu()].cpu().numpy(),\n",
    "        \"other_top_k_density\": feature_density_11[end_topk_ind.cpu()].cpu().numpy(),\n",
    "        \"feature_density_ratio\": feature_density_10[start_topk_ind.cpu()].cpu().numpy() / feature_density_11[end_topk_ind.cpu()].cpu().numpy(),\n",
    "    }\n",
    ")\n",
    "\n",
    "df.head()\n",
    "# # %% d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neel.utils as nutils\n",
    "token_df = nutils.make_token_df(toks_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_acts_10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_id_1 = 4315\n",
    "token_df[\"layer_10_feature_val\"] = feature_acts_10[:, feature_id_1].detach().cpu().numpy()\n",
    "feature_id_2 = 4399\n",
    "token_df[\"layer_11_feature_val\"] = feature_acts_11[:, feature_id_2].detach().cpu().numpy()\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "display(token_df.sort_values(\"layer_10_feature_val\", ascending=False).head(50))\n",
    "display(token_df.sort_values(\"layer_11_feature_val\", ascending=False).head(50))\n",
    "\n",
    "px.scatter(token_df[(token_df.layer_10_feature_val >0) | (token_df.layer_11_feature_val >0) ], x=\"layer_10_feature_val\", y=\"layer_11_feature_val\", hover_data=[\"str_tokens\", \"context\"], title=\"Feature 1 vs Feature 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "px.scatter(token_df[(token_df.layer_10_feature_val >0) | (token_df.layer_11_feature_val >0) ], x=\"layer_10_feature_val\", y=\"layer_11_feature_val\", hover_data=[\"str_tokens\", \"context\"], title=\"Feature 1 vs Feature 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_10.W_dec[start_topk_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cosine_similarity(encoder_10.W_dec[start_topk_ind], encoder_11.W_dec[end_topk_ind], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_11.W_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_topk_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae_2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
